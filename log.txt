Logging to /home/datasets/samples/
creating model and diffusion...
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Namespace(g_equiv=True, g_input='Z2', g_output='Z2', training_mode='edm', generator='determ', clip_denoised=True, num_samples=64, batch_size=64, sampler='heun', s_churn=0.0, s_tmin=0.0, s_tmax=inf, s_noise=1.0, steps=40, model_path='/checkpoint/dummy/-1/model001000.pt', seed=42, ts='', sigma_min=0.002, sigma_max=80.0, image_size=28, num_channels=64, num_res_blocks=1, num_heads=4, num_heads_upsample=-1, num_head_channels=32, attention_resolutions='32,16,8', channel_mult='', dropout=0.0, class_cond=False, use_checkpoint=False, use_scale_shift_norm=False, resblock_updown=False, use_fp16=False, use_new_attention_order=False, learn_sigma=False, weight_schedule='karras')
{'g_equiv': True, 'g_input': 'Z2', 'g_output': 'Z2', 'sigma_min': 0.002, 'sigma_max': 80.0, 'image_size': 28, 'num_channels': 64, 'num_res_blocks': 1, 'num_heads': 4, 'num_heads_upsample': -1, 'num_head_channels': 32, 'attention_resolutions': '32,16,8', 'channel_mult': '', 'dropout': 0.0, 'class_cond': False, 'use_checkpoint': False, 'use_scale_shift_norm': False, 'resblock_updown': False, 'use_fp16': False, 'use_new_attention_order': False, 'learn_sigma': False, 'weight_schedule': 'karras'}
UNetModel(
  (time_embed): Sequential(
    (0): Linear(in_features=64, out_features=256, bias=True)
    (1): SiLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (input_blocks): ModuleList(
    (0): TimestepEmbedSequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 64, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=64, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 64, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Identity()
      )
      (1): GAttentionBlock(
        (norm): GroupNorm32(32, 64, eps=1e-05, affine=True)
        (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (2): TimestepEmbedSequential(
      (0): GDownsample(
        (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (3): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 64, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (4): TimestepEmbedSequential(
      (0): GDownsample(
        (op): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (5): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=1024, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (middle_block): TimestepEmbedSequential(
    (0): GResBlock(
      (in_layers): Sequential(
        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=256, out_features=1024, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.0, inplace=False)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
    )
    (1): GAttentionBlock(
      (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)
      (qkv): Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))
      (attention): QKVAttentionLegacy()
      (proj_out): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): GResBlock(
      (in_layers): Sequential(
        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=256, out_features=1024, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0.0, inplace=False)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
    )
  )
  (output_blocks): ModuleList(
    (0): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 2048, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=1024, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (1): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(1280, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=1024, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(1280, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): GUpsample(
        (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (2): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(1280, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (3): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): GUpsample(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (4): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=64, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 64, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): GAttentionBlock(
        (norm): GroupNorm32(32, 64, eps=1e-05, affine=True)
        (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (5): TimestepEmbedSequential(
      (0): GResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=256, out_features=64, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 64, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): GAttentionBlock(
        (norm): GroupNorm32(32, 64, eps=1e-05, affine=True)
        (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))
        (attention): QKVAttentionLegacy()
        (proj_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (out): Sequential(
    (0): GroupNorm32(32, 64, eps=1e-05, affine=True)
    (1): SiLU()
    (2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
